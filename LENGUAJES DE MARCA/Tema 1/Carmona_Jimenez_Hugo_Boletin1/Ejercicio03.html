<!DOCTYPE html>
<html lang es>
    <head>
        <title>Ejercicio 3</title>
        <meta charset="utf8">
    </head>
    <body>
        <h1>La fiabilidad de la Inteligencia Artificial ha empeorado: resuelve lo complicado, pero falla en lo fácil</h1>

        <ul>
            <li><h3>Se ha revelado que los resultados no son precisos al 100%, sobre todo en tareas sencillas, su punto débil.</h3></li>
            <li><h3>Este es el cambio histórico que llega a Google Maps, Earth y Street View y que podrá mejorar hasta tu casa.</h3></li>
        </ul>
        <hr>
        <p>Pese a que la Inteligencia Artificial cada día está más integrada en la sociedad y es una herramienta fundamental para muchos, <strong>sus nuevos modelos
        de lenguaje han empeorado y "no son tan fiables</strong> como los usuarios esperan", así lo ha revelado un estudio de la Universitat Politècnica de
        Valencia (UPV) y la Universidad de Cambridge.</p>

        <p>La investigación ha sido liderada por un equipo del Instituto VRAIN de la UPV y la Escuela Valenciana de Posgrado y Red de Investigación en Inteligencia
        Artificial (ValgrAI), junto con la Universidad de Cambridge y se ha publicado este jueves <strong>en la revista <em>Natura</em></strong>. Por su parte, uno de los
        investigadores, José Hernández Orallo, asegura que "una de las principales preocupaciones sobre la fiabilidad de los modelos de lenguaje es que su
        funcionamiento <strong>no se ajusta a la percepción humana</strong> de dificultad de la tarea".</p>

        <p>"Los modelos pueden resolver ciertas tareas complejas de acuerdo a las habilidades humanas, pero al mismo tiempo <strong>fallan en tareas simples</strong>
        del mismo dominio. Por ejemplo, pueden resolver varios problemas matemáticos de nivel de doctorado, pero <strong>se pueden equivocar en una simple suma</strong>
        ", ha apuntado Orallo. Otro de los expertos indica que "la calidad de los errores <strong>está aumentando</strong> y no disminuyendo" como muchos esperan.</p>

        <p>"Hay una discordancia entre las expectativas humanas y el rendimiento de estos modelos", recalca una de las investigadoras del proyecto, Yael Moros Daval,
        mientras otro de sus compañeros, Lexin Zhou, afirma sorprendido que el rendimiento en cuanto a <strong>tareas difíciles ha mejorado</strong>, pero que el de las
        tareas comunes y fáciles, empeora por momentos. </p>

        <p>El científico que está detrás de algunos de los mayores avances en inteligencia artificial de los últimos años como OpenAI, Ilya Sutskever predijo que
        "quizá con el tiempo <strong>esa discrepancia disminuiría</strong>". No obstante, este estudio ha demostrado que "no ha sido así". Para ello, las entidades
        investigaron algunos aspectos clave que afectan a la fiabilidad de los modelos de lenguaje desde una perspectiva humana.</p>
        <hr>
        <h2>La percepción de dificultad</h2>

        <p>El problema principal de estos nuevos lenguajes y actualizaciones es que <strong>no son precisos al 100%</strong> ni siquiera en tareas sencillas", algo que
        puede causar problemas de cara a los usuarios.</p>

        <p>"No existe una zona segura en la que se pueda confiar en que los modelos funcionen a la perfección", asegura. En la actualidad, los modelos de lenguaje
        recientes son mucho más propensos a proporcionar respuestas incorrectas, algo que podría decepcionar a muchos usuarios que "inicialmente <stong>confían
        demasiado en los modelos</stong>".</p>
        <hr>
        <h2>"Incapaz de compensar los problemas"</h2>

        <p>La investigación apunta que "es posible que la tendencia actual de progreso en el desarrollo de modelos de lenguaje y de mayor comprensión de una variedad de
        órdenes, <strong>no libere a los usuarios de preocuparse en hacer enunciados eficaces</strong>". "Los usuarios pueden <strong>dejarse influir</strong> por
        'prompts' que funcionan bien en tareas complejas pero que, al mismo tiempo, obtienen respuestas incorrectas en tareas sencillas", ha indicado Cèsar Ferri,
        uno de los investigadores de VRAI UPV.</p>

        <p>Además, han descubierto que la "supervisión humana <strong>es incapaz de compensar estos problemas</strong>". "Las personas pueden reconocer las tareas de
        alta dificultad, pero siguen considerando con frecuencia que los resultados incorrectos son correctos, incluso cuando se les permite decir 'no estoy
        seguro'", ha asegurado Ferri.</p>
        <hr>
        <h2>Desde ChatGPT a LLaMA y BLOOM</h2>

        <p>Los resultados fueron <strong>similares para múltiples familias de modelos</strong> de lenguaje, incluidos la familia GPT de OpenAI, LLaMA de pesos abiertos
        de Meta, y BLOOM, una iniciativa totalmente abierta de la comunidad científica. Los problemas de discordancia de dificultad, falta de abstención adecuada y
        sensibilidad al 'prompt' <strong>siguen siendo un problema</strong> para las nuevas versiones como los nuevos modelos o1 de OpenAI y Claude-3.5-Sonnet de
        Anthropic.</p>

        <p>La sociedad <strong>tiende "a confiar demasiado"</strong> en este tipo de modelos y es totalmente "necesario un cambio fundamental en el diseño y desarrollo
        de la IA de propósito general, sobre todo para las aplicaciones de alto riesgo, en las que la predicción del desempeño de los modelos de lenguaje como la
        detección de sus errores son primordiales", ha concluido el experto Wout Schellaert.</p>
    </body>
</html>